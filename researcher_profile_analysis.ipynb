{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84fab1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.51.1)\n",
      "Requirement already satisfied: sentence-transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.0.2)\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.10.1)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/fenilvadher/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/fenilvadher/Library/Python/3.11/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/fenilvadher/Library/Python/3.11/lib/python/site-packages (from sentence-transformers) (4.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/fenilvadher/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Downloading wordcloud-1.9.4-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m534.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m246.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl, wordcloud\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5 wordcloud-1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas requests nltk scikit-learn transformers sentence-transformers wordcloud matplotlib openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a661931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/fenilvadher/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fenilvadher/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Chen Jia...\n",
      "Error fetching data for Chen Jia\n",
      "Processing Aman Madaan...\n",
      "Error fetching data for Aman Madaan\n",
      "Processing Yunmo Chen...\n",
      "Error fetching data for Yunmo Chen\n",
      "Processing Nikolay Bogoychev...\n",
      "Error fetching data for Nikolay Bogoychev\n",
      "Processing Shuhuai Ren...\n",
      "Error fetching data for Shuhuai Ren\n",
      "Processing Tunga Gungor...\n",
      "Error fetching data for Tunga Gungor\n",
      "Processing Sheng Shen...\n",
      "Error fetching data for Sheng Shen\n",
      "Processing Xiang Li0...\n",
      "Error fetching data for Xiang Li0\n",
      "Processing Hao Tang...\n",
      "Error fetching data for Hao Tang\n",
      "Processing Zhixuan Zhou...\n",
      "Error fetching data for Zhixuan Zhou\n",
      "Processing Wei Bi...\n",
      "Error fetching data for Wei Bi\n",
      "Processing Da Yin...\n",
      "Error fetching data for Da Yin\n",
      "Processing Monjoy Saha...\n",
      "Error fetching data for Monjoy Saha\n",
      "Processing Kaushal Kumar Maurya...\n",
      "Error fetching data for Kaushal Kumar Maurya\n",
      "Processing Thang Vu...\n",
      "Error fetching data for Thang Vu\n",
      "Processing John Ortega...\n",
      "Error fetching data for John Ortega\n",
      "Processing Irina Temnikova...\n",
      "Error fetching data for Irina Temnikova\n",
      "Processing Nan Jiang...\n",
      "Error fetching data for Nan Jiang\n",
      "Processing Rujun Han...\n",
      "Error fetching data for Rujun Han\n",
      "Analysis complete. Outputs saved in 'researcher_profiles' directory and 'Researcher_Analysis.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from openpyxl import Workbook\n",
    "import uuid\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Configuration\n",
    "SERPAPI_KEY = \"your_serpapi_key_here\"  # Replace with your SerpAPI key\n",
    "RESEARCHERS = [\n",
    "    \"Chen Jia\", \"Aman Madaan\", \"Yunmo Chen\", \"Nikolay Bogoychev\", \"Shuhuai Ren\",\n",
    "    \"Tunga Gungor\", \"Sheng Shen\", \"Xiang Li0\", \"Hao Tang\", \"Zhixuan Zhou\",\n",
    "    \"Wei Bi\", \"Da Yin\", \"Monjoy Saha\", \"Kaushal Kumar Maurya\", \"Thang Vu\",\n",
    "    \"John Ortega\", \"Irina Temnikova\", \"Nan Jiang\", \"Rujun Han\"\n",
    "]  # Your assigned researchers (excluding duplicates)\n",
    "OUTPUT_DIR = \"researcher_profiles\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize NLP tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "keyword_extractor = pipeline(\"token-classification\", model=\"dslim/bert-base-NER\")\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def fetch_scholar_data(author_name):\n",
    "    \"\"\"Fetch 20 recent publications from Google Scholar using SerpAPI.\"\"\"\n",
    "    params = {\n",
    "        \"engine\": \"google_scholar\",\n",
    "        \"q\": f\"author:{author_name}\",\n",
    "        \"api_key\": SERPAPI_KEY,\n",
    "        \"num\": 20\n",
    "    }\n",
    "    response = requests.get(\"https://serpapi.com/search\", params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching data for {author_name}\")\n",
    "        return []\n",
    "    \n",
    "    data = response.json()\n",
    "    publications = data.get(\"organic_results\", [])\n",
    "    results = []\n",
    "    for pub in publications[:20]:\n",
    "        title = pub.get(\"title\", \"N/A\")\n",
    "        abstract = pub.get(\"snippet\", \"N/A\")  # Note: SerpAPI may not always provide full abstracts\n",
    "        results.append({\"title\": title, \"abstract\": abstract})\n",
    "    return results\n",
    "\n",
    "def extract_keywords(text):\n",
    "    \"\"\"Extract keywords using TF-IDF and BERT NER.\"\"\"\n",
    "    # TF-IDF keywords\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "    tfidf_keywords = sorted([(feature_names[i], tfidf_scores[i]) for i in range(len(feature_names))], key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    # BERT NER for entities\n",
    "    entities = keyword_extractor(text)\n",
    "    ner_keywords = [entity['word'] for entity in entities if entity['entity'].startswith('B-') and entity['word'].lower() not in stop_words]\n",
    "    \n",
    "    return list(set([kw[0] for kw in tfidf_keywords] + ner_keywords))\n",
    "\n",
    "def compute_diversity(abstracts):\n",
    "    \"\"\"Compute research diversity using sentence embeddings.\"\"\"\n",
    "    if len(abstracts) < 2:\n",
    "        return 0.0\n",
    "    embeddings = sentence_model.encode(abstracts, convert_to_tensor=True)\n",
    "    similarity_scores = []\n",
    "    for i in range(len(abstracts)):\n",
    "        for j in range(i + 1, len(abstracts)):\n",
    "            sim = util.cos_sim(embeddings[i], embeddings[j]).item()\n",
    "            similarity_scores.append(sim)\n",
    "    return sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0.0\n",
    "\n",
    "def generate_wordcloud(text, filename):\n",
    "    \"\"\"Generate and save a Word Cloud.\"\"\"\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stop_words).generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def process_researcher(researcher):\n",
    "    \"\"\"Process a single researcher's data.\"\"\"\n",
    "    print(f\"Processing {researcher}...\")\n",
    "    publications = fetch_scholar_data(researcher)\n",
    "    \n",
    "    # Create Excel sheet for researcher\n",
    "    df = pd.DataFrame({\n",
    "        \"S.No\": range(1, len(publications) + 1),\n",
    "        \"Researcher Name\": [researcher] * len(publications),\n",
    "        \"Title of the Paper\": [pub[\"title\"] for pub in publications],\n",
    "        \"Abstract\": [pub[\"abstract\"] for pub in publications]\n",
    "    })\n",
    "    df.to_excel(f\"{OUTPUT_DIR}/{researcher.replace(' ', '_')}.xlsx\", index=False)\n",
    "    \n",
    "    # Combine abstracts for analysis\n",
    "    combined_abstracts = \" \".join([pub[\"abstract\"] for pub in publications if pub[\"abstract\"] != \"N/A\"])\n",
    "    if not combined_abstracts:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Extract research themes\n",
    "    keywords = extract_keywords(combined_abstracts)\n",
    "    themes = \", \".join(keywords[:5])  # Top 5 themes\n",
    "    \n",
    "    # Compute diversity\n",
    "    abstracts = [pub[\"abstract\"] for pub in publications if pub[\"abstract\"] != \"N/A\"]\n",
    "    avg_similarity = compute_diversity(abstracts)\n",
    "    diversity_score = \"High\" if avg_similarity < 0.4 else \"Medium\" if avg_similarity < 0.7 else \"Low\"\n",
    "    \n",
    "    # Generate Word Cloud\n",
    "    wordcloud_path = f\"{OUTPUT_DIR}/{researcher.replace(' ', '_')}_wordcloud.png\"\n",
    "    generate_wordcloud(combined_abstracts, wordcloud_path)\n",
    "    \n",
    "    return themes, avg_similarity, diversity_score\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process all researchers and generate outputs.\"\"\"\n",
    "    profile_data = []\n",
    "    diversity_data = []\n",
    "    \n",
    "    for researcher in RESEARCHERS:\n",
    "        themes, avg_similarity, diversity_score = process_researcher(researcher)\n",
    "        if themes:\n",
    "            profile_data.append({\"Researcher\": researcher, \"Top Research Themes\": themes})\n",
    "            diversity_data.append({\"Researcher\": researcher, \"Average Similarity\": round(avg_similarity, 2), \"Diversity Score\": diversity_score})\n",
    "    \n",
    "    # Create summary Excel file\n",
    "    with pd.ExcelWriter(\"Researcher_Analysis.xlsx\") as writer:\n",
    "        # Write individual researcher sheets (already saved)\n",
    "        # Write Author_Profiles summary\n",
    "        pd.DataFrame(profile_data).to_excel(writer, sheet_name=\"Author_Profiles\", index=False)\n",
    "        # Write Author_Diversity summary\n",
    "        pd.DataFrame(diversity_data).to_excel(writer, sheet_name=\"Author_Diversity\", index=False)\n",
    "    \n",
    "    print(\"Analysis complete. Outputs saved in 'researcher_profiles' directory and 'Researcher_Analysis.xlsx'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46b9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
